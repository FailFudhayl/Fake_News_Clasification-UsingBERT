{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\msi bravo\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fake_news_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Foreign Democrat final.</td>\n",
       "      <td>more tax development both store agreement lawy...</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Paula George</td>\n",
       "      <td>Politics</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To offer down resource great point.</td>\n",
       "      <td>probably guess western behind likely next inve...</td>\n",
       "      <td>2022-05-25</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Joseph Hill</td>\n",
       "      <td>Politics</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Himself church myself carry.</td>\n",
       "      <td>them identify forward present success risk sev...</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Julia Robinson</td>\n",
       "      <td>Business</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You unit its should.</td>\n",
       "      <td>phone which item yard Republican safe where po...</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Mr. David Foster DDS</td>\n",
       "      <td>Science</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Billion believe employee summer how.</td>\n",
       "      <td>wonder myself fact difficult course forget exa...</td>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Austin Walker</td>\n",
       "      <td>Technology</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title  \\\n",
       "0               Foreign Democrat final.   \n",
       "1   To offer down resource great point.   \n",
       "2          Himself church myself carry.   \n",
       "3                  You unit its should.   \n",
       "4  Billion believe employee summer how.   \n",
       "\n",
       "                                                text        date    source  \\\n",
       "0  more tax development both store agreement lawy...  2023-03-10  NY Times   \n",
       "1  probably guess western behind likely next inve...  2022-05-25  Fox News   \n",
       "2  them identify forward present success risk sev...  2022-09-01       CNN   \n",
       "3  phone which item yard Republican safe where po...  2023-02-07   Reuters   \n",
       "4  wonder myself fact difficult course forget exa...  2023-04-03       CNN   \n",
       "\n",
       "                 author    category label  \n",
       "0          Paula George    Politics  real  \n",
       "1           Joseph Hill    Politics  fake  \n",
       "2        Julia Robinson    Business  fake  \n",
       "3  Mr. David Foster DDS     Science  fake  \n",
       "4         Austin Walker  Technology  fake  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   title     20000 non-null  str  \n",
      " 1   text      20000 non-null  str  \n",
      " 2   date      20000 non-null  str  \n",
      " 3   source    19000 non-null  str  \n",
      " 4   author    19000 non-null  str  \n",
      " 5   category  20000 non-null  str  \n",
      " 6   label     20000 non-null  str  \n",
      "dtypes: str(7)\n",
      "memory usage: 33.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>19000</td>\n",
       "      <td>19000</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>1096</td>\n",
       "      <td>8</td>\n",
       "      <td>17051</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Foreign Democrat final.</td>\n",
       "      <td>more tax development both store agreement lawy...</td>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>Daily News</td>\n",
       "      <td>Michael Smith</td>\n",
       "      <td>Health</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2439</td>\n",
       "      <td>12</td>\n",
       "      <td>2922</td>\n",
       "      <td>10056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title  \\\n",
       "count                     20000   \n",
       "unique                    20000   \n",
       "top     Foreign Democrat final.   \n",
       "freq                          1   \n",
       "\n",
       "                                                     text        date  \\\n",
       "count                                               20000       20000   \n",
       "unique                                              20000        1096   \n",
       "top     more tax development both store agreement lawy...  2023-08-31   \n",
       "freq                                                    1          32   \n",
       "\n",
       "            source         author category  label  \n",
       "count        19000          19000    20000  20000  \n",
       "unique           8          17051        7      2  \n",
       "top     Daily News  Michael Smith   Health   fake  \n",
       "freq          2439             12     2922  10056  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation & Deep Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksplorasi & Label Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita muat dataset dan gabungkan fitur teksnya. Menggabungkan title dan text seringkali meningkatkan akurasi karena model mendapatkan konteks penuh sejak dari judul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitur Engineering Sederhana\n",
    "# Kita gabungkan judul dan isi berita agar model paham konteks utuhnya\n",
    "df['combined_text'] = df['title'] + \" [SEP] \" + df['text'] \n",
    "# [SEP] adalah token khusus BERT untuk memisahkan dua bagian teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: 20000 baris\n",
      "                                       combined_text  label\n",
      "0  Foreign Democrat final. [SEP] more tax develop...      0\n",
      "1  To offer down resource great point. [SEP] prob...      1\n",
      "2  Himself church myself carry. [SEP] them identi...      1\n",
      "3  You unit its should. [SEP] phone which item ya...      1\n",
      "4  Billion believe employee summer how. [SEP] won...      1\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding\n",
    "df['label'] = df['label'].map({'real': 0, 'fake': 1})\n",
    "\n",
    "print(f\"Data ready: {df.shape[0]} baris\")\n",
    "print(df[['combined_text', 'label']].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT tidak membaca kata per kata seperti manusia, melainkan menggunakan Subword Tokenization.\n",
    "\n",
    "Konsep: WordPiece Tokenization\n",
    "\n",
    "Jika ada kata asing seperti \"internship\", BERT mungkin memecahnya menjadi intern dan ##ship. Ini memastikan tidak ada kata yang \"unknown\" (OOV - Out of Vocabulary)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selain memecah kata, Tokenizer BERT menghasilkan tiga hal penting:\n",
    "\n",
    "1. Input IDs: Representasi angka unik untuk setiap token.\n",
    "\n",
    "2. Attention Mask: Deretan angka 0 dan 1. Angka 1 berarti itu kata asli, 0 berarti itu padding (kosong). Ini memberitahu model: \"Hanya perhatikan angka 1, abaikan angka 0\".\n",
    "\n",
    "3. Special Tokens: BERT butuh token [CLS] di awal kalimat untuk klasifikasi dan [SEP] untuk pemisah."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kita akan gunakan library transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
      "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\msi bravo\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typer-slim (from transformers)\n",
      "  Downloading typer_slim-0.23.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.2.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.12.2)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.4.26)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting typer>=0.23.0 (from typer-slim->transformers)\n",
      "  Downloading typer-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer>=0.23.0->typer-slim->transformers) (8.2.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer>=0.23.0->typer-slim->transformers) (14.0.0)\n",
      "Collecting annotated-doc>=0.0.2 (from typer>=0.23.0->typer-slim->transformers)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\msi bravo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (0.1.2)\n",
      "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
      "   ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.3 MB 1.4 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.5/10.3 MB 1.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.8/10.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.3/10.3 MB 1.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.6/10.3 MB 1.3 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.8/10.3 MB 1.3 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.1/10.3 MB 1.4 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.4/10.3 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 2.6/10.3 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 2.9/10.3 MB 1.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.1/10.3 MB 1.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.4/10.3 MB 1.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.7/10.3 MB 1.3 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 3.9/10.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.2/10.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.5/10.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 4.7/10.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.0/10.3 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.2/10.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.8/10.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.0/10.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.3/10.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 6.6/10.3 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.8/10.3 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.1/10.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 7.3/10.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.6/10.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.9/10.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.1/10.3 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.4/10.3 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.7/10.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.9/10.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.2/10.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.4/10.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.7/10.3 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.0/10.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.3/10.3 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "   ---------------------------------------- 0.0/553.3 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 262.1/553.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 553.3/553.3 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.9 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.0/2.9 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.3/2.9 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.6/2.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.8/2.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.1/2.9 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.4/2.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.7 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.0/2.7 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.3/2.7 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.6/2.7 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.8/2.7 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.8/2.7 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.4/2.7 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.4/2.7 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 1.3 MB/s eta 0:00:00\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading pyyaml-6.0.3-cp313-cp313-win_amd64.whl (154 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.23.0-py3-none-any.whl (3.4 kB)\n",
      "Downloading typer-0.23.0-py3-none-any.whl (56 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Installing collected packages: shellingham, safetensors, pyyaml, hf-xet, h11, anyio, annotated-doc, httpcore, typer, httpx, typer-slim, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ----- ----------------------------------  2/14 [pyyaml]\n",
      "   ----- ----------------------------------  2/14 [pyyaml]\n",
      "   ----------- ----------------------------  4/14 [h11]\n",
      "   -------------- -------------------------  5/14 [anyio]\n",
      "   -------------- -------------------------  5/14 [anyio]\n",
      "   -------------------- -------------------  7/14 [httpcore]\n",
      "   -------------------- -------------------  7/14 [httpcore]\n",
      "   ---------------------- -----------------  8/14 [typer]\n",
      "   ------------------------- --------------  9/14 [httpx]\n",
      "   ------------------------- --------------  9/14 [httpx]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ---------------------------------- ----- 12/14 [tokenizers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ---------------------------------------- 14/14 [transformers]\n",
      "\n",
      "Successfully installed annotated-doc-0.0.4 anyio-4.12.1 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.4.1 pyyaml-6.0.3 safetensors-0.7.0 shellingham-1.5.4 tokenizers-0.22.2 transformers-5.1.0 typer-0.23.0 typer-slim-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI BRAVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Memanggil tokenizer yang sudah dilatih oleh Google\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        padding='max_length',     # Menyamakan panjang semua kalimat (misal 128 kata)\n",
    "        truncation=True,        # Memotong teks jika lebih dari max_length\n",
    "        max_length=128,         # Batas kata agar memori GPU tidak bengkak\n",
    "        return_tensors=\"pt\"     # Mengembalikan format PyTorch Tensor\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[ 101, 4911, 2739, 1024, 7733, 2003, 2665,  999,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Contoh cara kerjanya pada satu kalimat\n",
    "sample_text = \"Breaking news: Mars is green!\"\n",
    "encoded = preprocess_function(sample_text)\n",
    "\n",
    "print(f\"Token IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention Mask: {encoded['attention_mask']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['combined_text'], \n",
    "    df['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df['label'] # Menjaga keseimbangan porsi label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    8045\n",
       "0    7955\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    2011\n",
       "0    1989\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
