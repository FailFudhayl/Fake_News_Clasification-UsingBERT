{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI BRAVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fake_news_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Foreign Democrat final.</td>\n",
       "      <td>more tax development both store agreement lawy...</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Paula George</td>\n",
       "      <td>Politics</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To offer down resource great point.</td>\n",
       "      <td>probably guess western behind likely next inve...</td>\n",
       "      <td>2022-05-25</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Joseph Hill</td>\n",
       "      <td>Politics</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Himself church myself carry.</td>\n",
       "      <td>them identify forward present success risk sev...</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Julia Robinson</td>\n",
       "      <td>Business</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You unit its should.</td>\n",
       "      <td>phone which item yard Republican safe where po...</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Mr. David Foster DDS</td>\n",
       "      <td>Science</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Billion believe employee summer how.</td>\n",
       "      <td>wonder myself fact difficult course forget exa...</td>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Austin Walker</td>\n",
       "      <td>Technology</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title  \\\n",
       "0               Foreign Democrat final.   \n",
       "1   To offer down resource great point.   \n",
       "2          Himself church myself carry.   \n",
       "3                  You unit its should.   \n",
       "4  Billion believe employee summer how.   \n",
       "\n",
       "                                                text        date    source  \\\n",
       "0  more tax development both store agreement lawy...  2023-03-10  NY Times   \n",
       "1  probably guess western behind likely next inve...  2022-05-25  Fox News   \n",
       "2  them identify forward present success risk sev...  2022-09-01       CNN   \n",
       "3  phone which item yard Republican safe where po...  2023-02-07   Reuters   \n",
       "4  wonder myself fact difficult course forget exa...  2023-04-03       CNN   \n",
       "\n",
       "                 author    category label  \n",
       "0          Paula George    Politics  real  \n",
       "1           Joseph Hill    Politics  fake  \n",
       "2        Julia Robinson    Business  fake  \n",
       "3  Mr. David Foster DDS     Science  fake  \n",
       "4         Austin Walker  Technology  fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   title     20000 non-null  str  \n",
      " 1   text      20000 non-null  str  \n",
      " 2   date      20000 non-null  str  \n",
      " 3   source    19000 non-null  str  \n",
      " 4   author    19000 non-null  str  \n",
      " 5   category  20000 non-null  str  \n",
      " 6   label     20000 non-null  str  \n",
      "dtypes: str(7)\n",
      "memory usage: 33.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>19000</td>\n",
       "      <td>19000</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>1096</td>\n",
       "      <td>8</td>\n",
       "      <td>17051</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Foreign Democrat final.</td>\n",
       "      <td>more tax development both store agreement lawy...</td>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>Daily News</td>\n",
       "      <td>Michael Smith</td>\n",
       "      <td>Health</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2439</td>\n",
       "      <td>12</td>\n",
       "      <td>2922</td>\n",
       "      <td>10056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title  \\\n",
       "count                     20000   \n",
       "unique                    20000   \n",
       "top     Foreign Democrat final.   \n",
       "freq                          1   \n",
       "\n",
       "                                                     text        date  \\\n",
       "count                                               20000       20000   \n",
       "unique                                              20000        1096   \n",
       "top     more tax development both store agreement lawy...  2023-08-31   \n",
       "freq                                                    1          32   \n",
       "\n",
       "            source         author category  label  \n",
       "count        19000          19000    20000  20000  \n",
       "unique           8          17051        7      2  \n",
       "top     Daily News  Michael Smith   Health   fake  \n",
       "freq          2439             12     2922  10056  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation & Deep Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksplorasi & Label Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita muat dataset dan gabungkan fitur teksnya. Menggabungkan title dan text seringkali meningkatkan akurasi karena model mendapatkan konteks penuh sejak dari judul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitur Engineering Sederhana\n",
    "# Kita gabungkan judul dan isi berita agar model paham konteks utuhnya\n",
    "df['combined_text'] = df['title'] + \" [SEP] \" + df['text'] \n",
    "# [SEP] adalah token khusus BERT untuk memisahkan dua bagian teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: 20000 baris\n",
      "                                       combined_text  label_idx\n",
      "0  Foreign Democrat final. [SEP] more tax develop...          0\n",
      "1  To offer down resource great point. [SEP] prob...          1\n",
      "2  Himself church myself carry. [SEP] them identi...          1\n",
      "3  You unit its should. [SEP] phone which item ya...          1\n",
      "4  Billion believe employee summer how. [SEP] won...          1\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding\n",
    "df['label_idx'] = df['label'].map({'real': 0, 'fake': 1})\n",
    "\n",
    "print(f\"Data ready: {df.shape[0]} baris\")\n",
    "print(df[['combined_text', 'label_idx']].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT tidak membaca kata per kata seperti manusia, melainkan menggunakan Subword Tokenization.\n",
    "\n",
    "Konsep: WordPiece Tokenization\n",
    "\n",
    "Jika ada kata asing seperti \"internship\", BERT mungkin memecahnya menjadi intern dan ##ship. Ini memastikan tidak ada kata yang \"unknown\" (OOV - Out of Vocabulary)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selain memecah kata, Tokenizer BERT menghasilkan tiga hal penting:\n",
    "\n",
    "1. Input IDs: Representasi angka unik untuk setiap token.\n",
    "\n",
    "2. Attention Mask: Deretan angka 0 dan 1. Angka 1 berarti itu kata asli, 0 berarti itu padding (kosong). Ini memberitahu model: \"Hanya perhatikan angka 1, abaikan angka 0\".\n",
    "\n",
    "3. Special Tokens: BERT butuh token [CLS] di awal kalimat untuk klasifikasi dan [SEP] untuk pemisah."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kita akan gunakan library transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Memanggil tokenizer yang sudah dilatih oleh Google\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        padding='max_length',     # Menyamakan panjang semua kalimat (misal 128 kata)\n",
    "        truncation=True,        # Memotong teks jika lebih dari max_length\n",
    "        max_length=128,         # Batas kata agar memori GPU tidak bengkak\n",
    "        return_tensors=\"pt\"     # Mengembalikan format PyTorch Tensor\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[ 101, 4911, 2739, 1024, 7733, 2003, 2665,  999,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Contoh cara kerjanya pada satu kalimat\n",
    "sample_text = \"Breaking news: Mars is green!\"\n",
    "encoded = preprocess_function(sample_text)\n",
    "\n",
    "print(f\"Token IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention Mask: {encoded['attention_mask']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['combined_text'], \n",
    "    df['label_idx'], \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df['label_idx']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_idx\n",
       "1    8045\n",
       "0    7955\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_idx\n",
       "1    2011\n",
       "0    1989\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat Custom Dataset & DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konsep: PyTorch Dataset\n",
    "Bayangkan Dataset sebagai sebuah gudang yang menyimpan data, dan DataLoader sebagai kurir yang mengantarkan data dalam porsi kecil (Batch) ke model.\n",
    "\n",
    "Batch Size: Kita akan mengirim data, misalnya 16 atau 32 baris sekali jalan.\n",
    "\n",
    "Iterasi: Model akan belajar dari batch demi batch sampai seluruh data habis (1 Epoch)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita akan membuat kelas Python untuk membungkus data agar sesuai dengan standar PyTorch dan Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels.to_numpy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts[item]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mendefinisikan Arsitektur Model (Fine-Tuning BERT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menambahkan satu lapisan terakhir (Classification Head) yang tugasnya cuma satu: menentukan apakah fitur yang ditangkap BERT itu mengarah ke \"Fake\" atau \"Real\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 346.47it/s, Materializing param=bert.pooler.dense.weight]                               \n",
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_ds = FakeNewsDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "test_ds = FakeNewsDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdamW sering digunakan sebagai optimizer untuk BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning Rate (LR): Kecepatan belajar. Kita gunakan nilai yang sangat kecil (misal: 2e-5) karena kita tidak ingin merusak pengetahuan bahasa yang sudah dimiliki BERT, kita hanya ingin memolesnya sedikit.\n",
    "\n",
    "- Epochs: Berapa kali model akan melihat seluruh dataset. Untuk BERT, biasanya 3-4 kali sudah cukup."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 / 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 86/1000 [11:26<2:03:00,  8.08s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1} / {EPOCHS} ---\")\n",
    "    \n",
    "    # FASE TRAINING\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip gradient\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # FASE EVALUASI\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"\\nHasil Evaluasi:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=['Real', 'Fake']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPAN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './model_save_hoaks/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"\\nModel sukses disimpan di {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
